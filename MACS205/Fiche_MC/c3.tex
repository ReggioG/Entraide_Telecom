\noindent Le contexte est comme Monte-Carlo avec une variable observée en plus : $((X_1,Z_1), \ldots, (X_n,Z_n))$ i.i.d dans $S \times \R$, $X_1 \sim \mu$ et $\esp[Z_1]$ est connu.
Soit $\varphi \colon S \to \R$ tel que $\esp \abs{\varphi(X_1)} < \infty$, on cherche $I_\mu = \esp[\varphi(X_1)]$.

On peut se ramener à $\esp Z_1 = 0$, et on pose $\hat{I}_n^{(cv)} = \frac{1}{n} \sum_{i = 1}^n \left( \varphi(X_i) - Z_i \right)$.

\begin{pop}
	Si $\esp \abs{\varphi(X_1)} < \infty$ et $\esp \abs{Z_1} < \infty$, $\hat{I}_n^{(cv)}$ est sans biais et fortement consistant.
	Si de plus $\esp \left[ \abs{\varphi(X_1)}^2 \right] < \infty$ et $\esp \left[ \abs{Z_1}^2 \right] < \infty$ alors :
	\begin{itemize}
		\item[\textbullet] $\Var \left( \hat{I}_n^{(cv)} \right) = \frac{1}{n} \Var(\varphi(X_1) - Z_1)$ et $\hat{I}_n^{(cv)}$ est asymptotiquement normal avec variance $\sigma^2 = \Var(\varphi(X_1) - Z_1)$, i.e $\sqrt{n} \left( \hat{I}_n^{(cv)} - I \right) \overset{\mathcal{L}}{\longrightarrow} \mathcal{N}(0,\sigma^2)$,
		\item[\textbullet] un estimateur consistant de $\sigma^2$ est $\hat{\sigma}^2 = \frac{1}{n - 1} \sum_{i = 1}^n \left( (\varphi(X_i) - Z_i) - \hat{I}_n^{(cv)} \right)^2$.
	\end{itemize}
\end{pop}

%~ Pour un tel estimateur, le but est de réduire  ce risque $L^2$ au maximum en choisissant bien $Z_1$.
%~ Pour un tel estimateur, on obtient facilement les mêmes résultats que pour Monte-Carlo : forte consistance, normalité asymptotique et estimation consistante de la variance.

\begin{rem}
	Cela comprend Monte-Carlo : $Z_1 = 0$, et les variables antithétiques : $Z_1 = \frac{1}{2}(\varphi(X_1) - (\varphi \circ L)(X_1))$.
\end{rem}

\begin{rem}
	VC est plus performante que MC si $\Var(\varphi(X_1) - Z_1) \leq \Var(\varphi(X_1))$.
\end{rem}

Pour prévenir d'une mauvaise variable de contrôle, on définit l'estimateur $\frac{1}{n} \sum_{i = 1}^n (\varphi(X_i) - \beta Z_i)$, à utiliser si $\Var(\varphi(X_1) - \beta Z_1) \leq \Var(\varphi(X_1))$.
C'est vérifié avec $\beta^* = \argmin_\beta \Var \left( \varphi(X_1) - \beta Y_1 \right) = \esp[\varphi(X_1)Z_1]/\esp[Z_1^2]$.

%~ Soit $f_1,\ldots,f_m$ une collection de fonctions dont on connaît les intégrales.
%~ Supposons $\forall L \in \iniff{1}{m}, \int f_L \diff \lambda = 0$.
%~ Alors VC donne $\frac{1}{n} \sum_{i = 1}^n \left[ \varphi(u_i) - \sum_{j = 1}^m \beta_j f_j(u_i) \right]$.

%~ \begin{ex}
	%~ $(f_L)$ polynômes, $(f_L)$ base de Fourier ou $(f_L)$ indicatrices.
%~ \end{ex}


\subsection{Propriétés asymptotiques : $Z_1 \in \R^m$}

	On estime $I = \esp[\varphi(X_1)]$ par $\hat{I}_n^{(cv)}(\beta) = \frac{1}{n} \sum_{i = 1}^n \left( \varphi(X_i) - \transp{\beta}Z_i \right)$ où $\beta \in \R^m$ et $\esp Z_1 = 0$.

	La valeur théorique pour minimiser la variance, si $\esp \left[ Z_1 \transp{Z_1} \right]$ est inversible, est\linebreak $\beta^* = \esp \left[ Z_1 \transp{Z_1} \right]^{-1} \esp[Z_1 \varphi(X_1)]$.
	En pratique on l'estime.
	Notons $Z_{n,m} = \transp{\begin{pmatrix} Z_1 & \cdots & Z_n \end{pmatrix}} \in \R^{n \times m}$, et $\mathbf{\varphi}_i = \transp{\left(\varphi(X_1), \ldots, \varphi(X_n) \right)} \in \R^n$.
	
	Alors $\hat{\beta}_n = (\transp{Z_{n,m}} Z_{n,m})^{+} \transp{Z_{n,m}} \varphi_n = \left( \frac{1}{n} \sum Z_i \transp{Z_i} \right)^{-1} \frac{1}{n} \sum Z_i \varphi(X_i)$ (en notant $A^+$ l'inverse généralisé de $A$).
	
	\begin{pop}
		Supposons $\esp \abs{\varphi(X_1)} < \infty$, $\forall k \in \iniff{1}{m}, \esp \abs{\varphi(X_1) Z_{k,1}} < \infty$ et $\esp[Z_1 \transp{Z_1}]$ existe et est inversible.
		Alors $\hat{I}_n^{(cv)}(\hat{\beta}_n) \overset{\text{p.s.}}{\longrightarrow} I$ (fortement consistant).
		Si de plus $\esp \abs{\varphi(X_1)}^2 < \infty$, alors\linebreak $\sqrt{n} \left( \hat{I}_n^{(cv)}(\hat{\beta}_n) - I \right) \overset{\mathcal{L}}{\longrightarrow} \mathcal{N}(0,\sigma_m^2)$ avec $\sigma_m^2 = \Var \left( \varphi(X_1) - \transp{{\beta^*}} Z_1 \right)$.
	\end{pop}
	
	\begin{rem}
		L'estimation de $\hat{\beta}_n$ n'a pas d'effet en l'asymptotique (c'est comme si on connnaissait $\beta^*$).
	\end{rem}
	
	\begin{rem}
		D'autres estimateurs de $\beta^*$ peuvent être légitimes sous condition d'inversibilité et de consistance.
		%~ Lorsque $\esp[Z_1 \transp{Z_1}]$ est connu, $\hat{\beta} = \esp[Z_1 \transp{Z_1}]^{-1} \frac{1}{n} \sum_{i = 1}^n \left( Z_i (\varphi(X_i) - \bar{\varphi}) \right)$.
	\end{rem}
	
	\begin{rem}
		On a $\forall m \geq 0, \sigma_{m + 1} \leq \sigma_m$ et $\sigma_0^2$ correspond à la variance de Monte-Carlo.
		Un estimateur de la variance de $\hat{I}_n^{(cv)}(\hat{\beta}_n)$ est donné par $\hat{\sigma}^2 = \frac{1}{n} \sum_{i = 1}^n \left( \varphi(X_i) - \transp{\hat \beta_n} Z_i - \hat{I}_n^{(cv)}(\hat{\beta}_n) \right)^2$.
	\end{rem}
	
	\begin{pop}
		Supposons $\esp \abs{\varphi(X_1)}^2 < \infty$, $\forall k \in \iniff{1}{m}, \esp \abs{\varphi(X_1) Z_{k,1}} < \infty$ et $\esp[Z_1 \transp{Z_1}]$ est inversible.
		Alors $\hat{\sigma}^2 \underset{n \to \infty}{\longrightarrow} \sigma_m^2$.
	\end{pop}


\subsection{Complexité du calcul}

	Règles du temps de calcul : générer $X_1$, générer $Z_{1,k}$ pour un $k$ et évaluer $\varphi(X_1)$ comptent chacun pour une opération élémentaire.
	
	\begin{tabular}{ll}
	Méthode & Nombre d'opérations élémentaires \\
	\hline
	Monte Carlo & $O(n)$ \\
	Calcul de $\hat{\beta}_n$ & $O(m^2 n + m^3)$ \\
	Variables de contrôle (avec $\hat{\beta}_n$ donné) & $O(mn)$
	\end{tabular}
	
	Donc la méthode avec variable de contrôle est mieux que Monte Carlo lorsque $\frac{\sigma_m}{\sigma_0} \leq \frac{1}{m}$.