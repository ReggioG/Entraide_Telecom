\noindent Le contexte est comme Monte-Carlo avec une variable observée en plus : $((X_1,Z_1), \ldots, (X_n,Z_n))$ i.i.d dans $S \times \R$, $X_1 \sim \mu$ et $\esp[Z_1]$ est connu.
Soit $\varphi \colon S \to \R$ tel que $\esp \abs{\varphi(X_1)} < \infty$, on cherche $I_\mu = \esp[\varphi(X_1)]$.

On peut se ramener à $\esp Z_1 = 0$, et on pose $\hat{I}_n^{(cv)} = \frac{1}{n} \sum_{i = 1}^n \left( \varphi(X_i) - Z_i \right)$.

\begin{pop}
	Si $\esp \abs{\varphi(X_1)} < \infty$ et $\esp \abs{Z_1} < \infty$, $\hat{I}_n^{(cv)}$ est sans biais et fortement consistant.
	Si de plus $\esp \left[ \abs{\varphi(X_1)}^2 \right] < \infty$ et $\esp \left[ \abs{Z_1}^2 \right] < \infty$ alors :
	\begin{itemize}
		\item[\textbullet] $\Var \left( \hat{I}_n^{(cv)} \right) = \frac{1}{n} \Var(\varphi(X_1) - Z_1)$ et $\hat{I}_n^{(cv)}$ est asymptotiquement normal avec variance $\sigma^2 = \Var(\varphi(X_1) - Z_1)$, i.e $\sqrt{n} \left( \hat{I}_n^{(cv)} - I \right) \overset{\mathcal{L}}{\longrightarrow} \mathcal{N}(0,\sigma^2)$,
		\item[\textbullet] un estimateur consistant de $\sigma^2$ est $\hat{\sigma}^2 = \frac{1}{n - 1} \sum_{i = 1}^n \left( (\varphi(X_i) - Z_i) - \hat{I}_n^{(cv)} \right)^2$.
	\end{itemize}
\end{pop}

%~ Pour un tel estimateur, le but est de réduire  ce risque $L^2$ au maximum en choisissant bien $Z_1$.
%~ Pour un tel estimateur, on obtient facilement les mêmes résultats que pour Monte-Carlo : forte consistance, normalité asymptotique et estimation consistante de la variance.

\begin{rem}
	Cela comprend Monte-Carlo : $Z_1 = 0$, et les variables antithétiques : $Z_1 = \frac{1}{2}(\varphi(X_1) - (\varphi \circ L)(X_1))$.
\end{rem}

\begin{rem}
	VC est plus performante que MC si $\Var(\varphi(X_1) - Z_1) \leq \Var(\varphi(X_1))$.
\end{rem}

Pour prévenir d'une mauvaise variable de contrôle, on définit l'estimateur $\frac{1}{n} \sum_{i = 1}^n (\varphi(X_i) - \beta Z_i)$, à utiliser si $\Var(\varphi(X_1) - \beta Z_1) \leq \Var(\varphi(X_1))$.
C'est vérifié avec $\beta^* = \argmin_\beta \Var \left( \varphi(X_1) - \beta Y_1 \right) = \esp[\varphi(X_1)Z_1]/\esp[Z_1^2]$.

%~ Soit $f_1,\ldots,f_m$ une collection de fonctions dont on connaît les intégrales.
%~ Supposons $\forall L \in \iniff{1}{m}, \int f_L \diff \lambda = 0$.
%~ Alors VC donne $\frac{1}{n} \sum_{i = 1}^n \left[ \varphi(u_i) - \sum_{j = 1}^m \beta_j f_j(u_i) \right]$.

%~ \begin{ex}
	%~ $(f_L)$ polynômes, $(f_L)$ base de Fourier ou $(f_L)$ indicatrices.
%~ \end{ex}


\subsection{Propriétés asymptotiques, cas $Z_1 \in \R^m$}

	On définit l'estimateur de $\esp[\varphi(X_1)]$ par $\forall \beta \in \R^m, \hat{\mu}_n(\beta) = \frac{1}{n} \sum_{i = 1}^n \left( \varphi(X_i) - \transp{\beta}Y_i \right)$.

	Comme dans l'intro, on suppose $\esp Y_1 = \begin{pmatrix}
		\esp[Y_{1,1}] \\ \vdots \\ \esp[Y_{1,m}]
		\end{pmatrix} = 0$.
		
	$\{ \mu_n(\beta), \beta \in \R^m \}$ est une collection d'estimateurs sans biais.
	Trouvons l'élément de variance minimale :
	\begin{align*}
		\beta^* & = \argmin_\beta \frac{1}{n} \Var \left( \varphi - \transp{\beta} T \right) \\
		        & = \argmin_\beta \Var \left( \varphi - \transp{\beta} T \right) \\
		        & = \argmin_\beta \esp \left[ (\varphi - \transp{\beta} Y)^2 \right] - \esp[\varphi]^2 \\
		        & = \argmin_\beta \esp \left[ (\varphi - \transp{\beta} Y)^2 \right]
	\end{align*}
	Si $\esp[Y_1 \transp{Y_1}]$ est inversible, les équations normales / du premier ordre admettent une unique solution :
	$$\beta^* = \esp[Y_1 \transp{Y_1}]^{-1} \esp[Y_1 \varphi(X_1)]$$
	
	Il faut utiliser $\hat{\mu}_n(\beta^*)$, mais $\beta^*$ est inconnue.
	
	Idée : estimer $\beta^*$ sur les données \textrightarrow\ $\hat{\beta}$, et utiliser $\hat{\mu}_n(\hat{\beta})$, qui a la même variance asymptotique que $\hat{\mu}_n(\beta^*)$.
	
	Si $\frac{1}{n} \sum_{i = 1}^n Y_i \transp{Y_i}$ est inversible :
	$$\hat{\beta} = \argmin_{\beta \in \R^m}
	\underset{\text{estimateur classique de la covariance}}{\underbrace{\frac{1}{n} \sum_{i = 1}^n \left( [\varphi(X_i) - \transp{\beta} Y_i] - \hat{\mu}_n(\beta) \right)^2}}$$
	
	Ce choix ne va pas entrainer de changement à l'asymptotique mais pratique il procure de meilleurs performance.
	Donc $\hat{\beta} = \argmin_\beta \frac{1}{n} \sum_{i = 1}^n \left( (\varphi(X_i) - \bar{\varphi}) - \transp{\beta} (Y_i - \bar{Y}) \right)^2$.
	
	Notons
	$$Z_{n,m} = \begin{pmatrix}
		Y_{11} - \bar{Y}_1 & \cdots & Y_{1m} - \bar{Y}_m \\
		\vdots             &        & \vdots \\
		Y_{n1} - \bar{Y}_1 & \cdots & Y_{nm} - \bar{Y}_m
		\end{pmatrix} \in \R^{n \times m}, \qquad
		Y_i = \begin{pmatrix} Y_{i1} \\ \vdots \\ Y_{im} \end{pmatrix} \in \R^m$$
	($Y_i$ est la covariable du problème de régression).
	On a $\bar{Y}_k = \frac{1}{n} \sum_{i = 1}^n Y_{ik}$.
	
	Notons également $\Psi_n = \begin{pmatrix} \varphi(X_1) - \bar{\varphi} \\ \vdots \\ \varphi(X_n) - \bar{\varphi} \end{pmatrix}$.
	Alors $\hat{\beta} = \argmin_{\beta} \norme{\Psi_n - Z_{n,m}\beta}^2$.
	
	Le théorème de projection nous donne une unique solution qui, si $\transp{Z_{n,m}} Z_{n,m}$ est inversible, vérifie :
	$$(\transp{Z_{n,m}} Z_{n,m}) \beta = \transp{Z_{n,m}} \Psi_n$$
	$$\hat{\beta} = (\transp{Z_{n,m}} Z_{n,m})^{-1} \transp{Z_{n,m}} \Psi_n$$
	
	\begin{pop}[asymptotique de $\hat{\mu}_n(\beta)$]
		Supposons que $\esp \abs{\varphi(X_1)} < \infty$, $\forall k \in \iniff{1}{m}, \esp \abs{\varphi(X_1) Y_{1k}} < \infty$ et $\esp[Y_1 \transp{Y_1}]$ existe et est inversible.
		Alors $\hat{\mu}_n(\hat{\beta}) \overset{\text{p.s.}}{\longrightarrow} \esp[\varphi(X_1)]$.
		Si de plus $\esp \abs{\varphi(X_1)}^2 < \infty$, alors $\sqrt{n} \left( \hat{\mu}_n(\hat{\beta} - \esp[\varphi(X_1)] \right) \longrightarrow \normale(0,\sigma_m^2)$ avec $\sigma_m^2 = \Var \left( \varphi(X_1) - \transp{{\beta^*}} Y_1 \right)$.
	\end{pop}
	
	\begin{rem}
		$\hat{\beta}$ n'a pas d'effet en l'asymptotique (c'est comme si on connnaissait $\beta^*$).
	\end{rem}
	
	\begin{rem}
		D'autres estimateurs de $\beta^*$ peuvent être légitimes sous condition d'inversibilité :
		$$\hat{\beta} = \left\{ \begin{array}{l}
			\left( \frac{1}{n} \sum Y_i \transp{Y_i} \right)^{-1} \frac{1}{n} \sum Y_i \varphi(X_i) \\
			\left( \frac{1}{n} \sum (Y_i - \bar{Y}) \transp{(Y_i - \bar{Y})} \right)^{-1} \frac{1}{n} \sum Y_i \varphi(X_i)
			\end{array} \right.$$
		Lorsque $\esp[Y_1 \transp{Y_1}]$ est connu, $\hat{\beta} = \esp[Y_1 \transp{Y_1}]^{-1} \frac{1}{n} \sum_{i = 1}^n \left( Y_i (\varphi(X_i) - \bar{\varphi}) \right)$.
	\end{rem}
	
	...


\subsection{Temps de calcul}

	Soit $F$ une c.d.f sur $\R$ et $\varphi \colon \R \to \R$.
	On veut calculer $\esp_F[\varphi]$.
	
	Le nombre d'échantillons n'est pas fixé par le problème initial.
	Il est donc à déterminer par rapport à la précision souhaitée et le temps de calcul dont on dispose.
	
	Données massives \textrightarrow\ la problématique du temps de calcul est redevenue essentielle aujourd'hui.
	
	Mesure du temps \textrightarrow\ par simulation, en terme d'opérations élémentaires.
	
	Règles du temps de calcul (peuvent changer selon le problème) :
	\begin{itemize}
		\item[\textbullet] générer $X_1$ \textrightarrow\ 1 opération élémentaire,
		\item[\textbullet] générer $Y_{1,k}$ pour chaque $k$ \textrightarrow\ 1 opération élémentaire,
		\item[\textbullet] évaluer $\varphi(X_1)$ \textrightarrow\ 1 opération élémentaire.\\
	\end{itemize}
	
	\begin{tabular}{c|c}
	MC & nombre d'opérations élémentaires \\
	\hline
	$X_1,\ldots,X_n$ & $n$ \\
	$\varphi(X_1),\ldots,\varphi(X_n)$ & $n$ \\
	$\frac{1}{n} \sum_{i = 1}^n \varphi(X_i)$ & $\sim  n$\\
	 & $O(n)$\\
	\end{tabular}
	
	\begin{tabular}{cc}
	VC & nombre d'opérations élémentaires \\
	\hline
	$X_1,\ldots,X_n$ & $n$ \\
	$\varphi(X_1),\ldots,\varphi(X_n)$ & $n$ \\
	$Y_1,\ldots,Y_n$ & $mn$ \\
	$\frac{1}{n} \sum_{i = 1}^n (\varphi(X_i) - \transp{\beta} Y_i)$ & $mn$ \\
	 & $O(mn)$\\
	\end{tabular}
	
	\begin{tabular}{cc}
	$\hat{\beta}$ & nombre d'opérations élémentaires \\
	\hline
	... & ... \\
	... & ... \\
	... & .... \\
	& $O(m^3 + m^2n)$\\
	\end{tabular}