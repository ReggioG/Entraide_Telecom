\subsection{Généralités}

	Soit $(\Omega,\mathcal{F},\proba)$ un espace de probabilités.
	Soit $d \in \N^*$, $E = \R^d$ et $\mathcal{E} = \mathcal{B}(E)$.
	
	On note $\mu \colon B \mapsto \proba(X^{-1}(B))$ la loi de probabilité de $X$.
	
	Soit $\mathbf{T}$ un “ensemble d'indices” qui représente le temps.
	En général $\mathbf{T} = \R_+$.
	
	\begin{defn}
		Un processus à valeurs dans $(E,\mathcal{E})$ indexé par $\mathbf{T}$ est une famille de v.a $X = (X_t)_{t \in \mathbf{T}}$ à valeurs dans $(E,\mathcal{E})$.
		Pour tout $\omega \in \Omega$, l'application $t \mapsto X_t(\omega)$ est appelé \textbf{trajectoire} de $X$.
	\end{defn}
	
	La famille $X$ peut-être vue comme une application $\Omega \to E^{\mathbf{T}}$ de toutes les trajectoires possibles.
	Il faut donc définir une tribu sur $E^{\mathbf{T}}$ et caractériser la mesure.
	
	Soit $t \in \mathbf{T}$, on pose $\mathcal{G}_t := \sigma(\xi_t)$ la tribu sur $E^{\mathbf{T}}$ engendrée par la projection $\xi_t \colon \begin{array}{lcr} E^{\mathbf{T}} & \to & E \\ x & \mapsto & x(t) \end{array}$.
	Cette tribu est donc constituée des ensembles $\{ x \in E^{\mathbf{T}} \mid x(t) \in H \}$ où $H$ parcourt $\mathcal{E}$.
	
	\begin{defn}
		La \textbf{tribu de Kolmogorov} est la tribu $\mathcal{G}$ engendrée par la famille $\{ \mathcal{G}_t \}_{t \in \mathbf{T}}$.
	\end{defn}
	
	D'une manière équivalente, $\mathcal{G}$ est la plus petite tribu rendant mesurables toutes les applications $\xi_t$ où $t$ parcourt $\mathbf{T}$.
	Avec cette construction $X \colon \Omega \to E^{\mathbf{T}}$ est $\mathcal{F}/\mathcal{G}$-mesurable de loi $\mu$ l'image de $\proba$ par $X$.
	
	Étant donné une loi de probabilité $\mu$ sur $\left( E^{\mathbf{T}}, \mathcal{G} \right)$, il est facile de construire un processus de loi $\mu$ : il suffit de prendre $(\Omega, \mathcal{F}, \proba) = \left( E^{\mathbf{T}}, \mathcal{G}, \mu \right)$ et de poser $X(\omega) = \omega$.
	
	Ce processus est appelé \textbf{processus canonique}.
	
	\begin{defn}[\textbf{Lois fini-dimensionnelles}]
		Soit $\mathcal{J}$ l'ensemble des parties finies de $\mathbf{T}$ et $I = \{ t_1,\ldots,t_n \} \in \mathcal{J}$ où $t_1 < t_2 < \cdots < t_n$.
		Soit $\mu_I$ la loi du vecteur $(X_{t_1},\ldots,X_{t_n})$.
		En notant $\mathcal{G}_I := \sigma(\xi_I)$ la sous-tribu de $\mathcal{G}$ engendrée par $\xi_I \colon \begin{array}{lcr} E^{\mathbf{T}} & \to & E^I \\ x & \mapsto & (x(t_1),\ldots,x(t_n)) \end{array}$, la loi $\mu_I$ peut être définie sur $(E^I, \mathcal{G}_I)$ comme étant l'image de $\mu$ par $\xi_I$.
	\end{defn}
	
	\begin{rem}
		$\mathcal{G}_I$ est la collection des ensembles $\{ x \in E^{\mathbf{T}} \mid (x(t_1),\ldots,x(t_n)) \in H \}$ où $H \in \xi^{\otimes I}$ est la tribu produit sur $E^I$.
		Donc $\mathcal{G}_I$ peut être identifiée à $\mathcal{E}^{\otimes I}$ et on peut caractériser $\mu_I$ par $\forall H_1,\ldots,H_n \in \mathcal{E}, \mu_I(H_1 \times \cdots \times H_n) = \proba(X_{t_1} \in H_1, \ldots, X_{t_n} \in H_n)$.
	\end{rem}
	
	\begin{defn}
		La famille des lois fini-dimensionnelles de $X$ est la famille des $\mu_I$ où $I$ parcourt $\mathcal{J}$.
	\end{defn}
	
	\begin{pop}
		Si deux lois $\mu$ et $\nu$ sur $\left( E^{\mathbf{T}}, \mathcal{G} \right)$ possèdent les mêmes lois fini-dimensionnelles alors elles sont égales.
	\end{pop}
	
	\begin{proof}
		$\mathcal{G}$ est engendré par l'algèbre $\bigcup_{I \in \mathcal{J}} \mathcal{G}_I$.
		Comme $\mu$ et $\nu$ coïncident sur cette algèbre elles coïncident sur $\mathcal{G}$.
	\end{proof}
	
	\begin{pop}
		Les lois fini-dimensionnelles satisfont la \textbf{condition de compatibilité} suivante : pour tout $I = \{ t_1,\ldots,t_n \}$ avec $t_1 < \cdots < t_n$, pour $p \in \iniff{1}{n}$ et $J = \{ t_1,\ldots,t_{p - 1},t_{p + 1},\ldots,t_n \} \subset I$, pour toutes les familles  $(H_i)$ de $\mathcal{E}$, on a $\mu_I(H_1 \times \cdots H_{p - 1} \times E \times H_{p + 1} \times \cdots \times H_n) = \mu_J(H_1 \times \cdots H_n)$.
	\end{pop}
	
	\begin{thm}[\textbf{Kolmogorov}]
		Soit $(\mu_I)_{I \in \mathcal{J}}$ une famille de lois sur $\left( E^I, \mathcal{E}^{\otimes I} \right)_{I \in \mathcal{J}}$.
		Si elle vérifie les conditions de compatibilité, $(\mu_I)_{I \in \mathcal{J}}$ est la famille de lois fini-dimensionnelles d'une unique mesure de probabilités $\mu$ sur $\left( E^{\mathbf{T}}, \mathcal{G} \right)$.
	\end{thm}
	
	{\Large \noindent\lightning} Ici $E = \R^d$. Cela ne marche pas pour tous types de $E$.
	
	\begin{ex}
		Prenons $E = \R$.
		Soit $\nu$ une mesure sur $\R$.
		Supposons $\mu_I = \otimes^n \nu$, avec $n = \card(I)$.
		Alors il existe un processus aléatoire tel que ...
		TODO
	\end{ex}
	
	\begin{defn}
		Soit $X$ et $X'$ deux processus définis sur le même espace de probabilités.
		\begin{itemize}
			\item[\textbullet] On dit que $X'$ est une \textbf{modification} de $X$ si $\forall t \in \mathbf{T}, \proba(X_t = X_t') = 1$.
			\item[\textbullet] On dit que $X$ et $X'$ sont \textbf{indistinguables} si $\proba(\forall t \in \mathbf{T}, X_t = X_t') = 1$ en admettant que $\{ \forall t \in \mathbf{T}, X_t = X_t' \} \in \mathcal{F}$.
		\end{itemize}
	\end{defn}
	
	\begin{ex}
		Soit $\Omega = \mathbf{T} = \inff{0}{1}$, $\mathcal{F} = \mathcal{B}(\inff{0}{1})$, $\proba$ la mesure de Lebesgue sur $\iniff{0}{1}$ et $\forall t \in \mathbf{T}, X_t(\omega) = \delta_{t,\omega} = \indic_{ \{ t \} }(\omega)$ et $\forall t, X_t'(\omega) = 0$.
		Alors $\forall t \in \mathbf{T}, \proba( \omega \mid X_t(\omega) \neq X_t'(\omega) ) = \proba( \{ t \} ) = 0$ mais $\proba( \omega \mid \exists t \in \mathbf{T}, X_t(\omega) \neq X_t'(\omega) ) = \proba(\inff{0}{1}) = 1$.
	\end{ex}
	
	Question : peut-on trouver une condition sur $\mu$ qui rende le processsus $X$ continu, au moins avec la probabilité 1, i.e. “presque toutes les trajectoires sont continues”, si cela a un sens ?
	Non, comme le montr l'exemple précédent.
	En effet les lois fini-dimensionnelles de $X$ et $X'$ sont identiques.
	Donc $X$ et $X'$ ont la même loi $\mu$.
	
	Cet exemple montre que l'ensemble des processus continus n'est pas mesurable par la tribu de Kolmogorov.
	En effet, si $\cont(\inff{0}{1})$ était mesurable, on aurait $\mu \left( \cont(\inff{0}{1}) \right) = 1$ car $\mu$ est la loi de $X' \in \cont(\inff{0}{1})$.
	En même temps $\mu \left( \cont(\inff{0}{1}) \right) = 0$ car $\mu$ est la loi de $X$.


\subsection{Le mouvement brownien}

	\begin{defn}
		Un processus aléatoire est dit \textbf{gaussien} si toutes ses lois fini-dimensionnelles sont gaussiennes.
	\end{defn}
	
	\begin{defn}
		Un \textbf{mouvement brownien au sens large (MBL)} est un processus scalaire gaussien $X$ sur $\mathbf{T} = \R_+$ tel que $\forall t \in \mathbf{T}, \esp X_t = 0$ et $\forall t,s \in \mathbf{T}, \esp[X_t X_s] = \min(t,s)$.
	\end{defn}
	
	\begin{pop}
		Le MBL existe.
	\end{pop}
	
	\begin{proof}
		Il nous faudra prouver que les conditions de compatibilité sont satisfaites.
		Pour tout $I = \{ t_1,\ldots,t_n \}, t_1 < \cdots < t_n$ il nous suffira de prouver que $\mu_I$ est une loi de probabilité.
		Ainsi $\mu_J$ pour tout $J \subset I$ sera la marginale correspondante de $\mu_I$.
		Cela revient à prouver que $\Gamma := (t_i \wedge t_j)_{1 \leq i,j \leq n}$ est une matrice de covariance, i.e une matrice semi-définie positive.
		En effet, avec $t_0 := 0$, $\forall x = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$,
		$$\transp{x} \Gamma_I x = \sum_{i,j = 1}^n x_i x_j (t_i \wedge t_j)
		                        = \sum_{i,j = 1}^n x_i x_j \sum_{l = 1}^{i \wedge j} (t_l - t_{l - 1})
		                        = \sum_{l = 1}^n (t_l - t_{l - 1}) \left( \sum_{i = l}^n x_i \right)^2
		                        \geq 0$$
	\end{proof}
	
	\begin{defn}
		Soit $\sigma(X_t)$ la sous-tribu de $\mathcal{F}$ engendrée par la v.a $\xi_t \circ X$.
		La tribu engendrée par $\{ \sigma(X_s) \}_{0 \leq s \leq t}$, noté $\sigma(X_s, 0 \leq s \leq t)$ représente le \textbf{passé} de $X$ antérieur à $t$.
	\end{defn}
	
	\begin{pop}
		Un processus $X$ est un MBL si et seulement si il satisfait les conditions suivantes :
		\begin{enumerate}[(i)]
			\item Il est à accroissement indépendants, i.e $\forall s,t \geq 0, X_{t+s} - X_t$ est indépendant de $\sigma(X_u, 0 \leq u \leq t)$.
			\item Il est gaussien centré et $\forall t \geq 0, \esp[X_t^2] = t$.
		\end{enumerate}
		Par ailleurs les accroissements d'un MBL satisfont $\forall s,t \geq 0, X_{t+s} - X_t \overset{\mathcal{L}}{=} X_s - X_0 \overset{\mathcal{L}}{=} X_s \sim \mathcal{N}(0,s)$.
	\end{pop}
	
	\begin{proof}
		Si $X$ est un MBL, il suffit de prouver le premier point.
		Comme la loi de $X$ est caractérisée par les lois fini-dimensionnelles, il suffit de prouver $\forall t_0,\ldots,t_{n + 1}$ tel que $0 = t_0 < t_1 < \cdots < t_n = t < t_{n + 1} = t + 1$, la v.a $X_{t_{n + 1}} - X_{t_n}$ et le vecteur $(X_{t_0},\ldots,X_{t_n})$ sont indépendants comme $(X_{t_0},\ldots,X_{t_{n + 1}})$ est gaussien.
		
		Le vecteur $(X_{t_0},\ldots,X_{t_n},X_{t_{n + 1}} - X_{t_n})$ l'est par transformation linéaire, et il suffit de prouver la décorrélation $\forall i \in \iniff{0}{1}, \esp \left[ (X_{t_{n + 1}} - X_{t_n}) X_{t_i} \right] = 0$.
		C'est immédiat : $\esp \left[ X_{t_{n + 1}} X_{t_i} \right] - \esp \left[ X_{t_n} X_{t_i} \right] = t_{n + 1} \wedge t_i - t_n \wedge t_i = t_i - t_i = 0$.
		
		Réciproquement, si les deux points sont satisfaits, il suffit de prouver que $\esp \left[ X_{t + s} X_t \right] = t$.
		En effet $\esp \left[ X_{t + s} X_t \right] = \esp \left[ (X_{t + s} - X_t) X_t \right] + \esp \left[ X_t^2 \right] = \esp \left[ X_{t + s} - X_t \right] \esp \left[ X_t \right] + \esp \left[ X_t^2 \right] = \esp \left[ X_t^2 \right] = t$.
		
		Enfin on sait que $X_{t + s} - X_t$ est gaussienne et il est facile de vérifier qu'elle est centrée et de variance $s$.
	\end{proof}
	
	\begin{thm}[Kolmogorov]
		Soit $\mathbf{T}$ un intervalle de $\R$ et $(X_t)_{t \in \mathbf{T}}$ un processus à valeurs dans $E^{\mathbf{T}}$.
		Supposons $\exists \alpha,\beta \in \R_+^*, \exists C > 0, \forall s,t \in \mathbf{T}, \esp \left[ \norme{X_t - X_s}^\beta \right] \leq C \abs{t - s}^{1 + \alpha}$.
		Alors $X$ admet une modification $\tilde{X} = \left( \tilde{X}_t \right)_{t \in \mathbf{T}}$ dont toutes les trajectoires $t \mapsto \tilde{X}_t(\omega)$ sont continues.
	\end{thm}
	
	\begin{defn}
		Un \textbf{mouvement brownien (MB)} ou processus de Wiener est un MBL dont toutes les trajectoires sont continues et nulles en $t = 0$.
	\end{defn}
	
	\begin{pop}
		Le MB existe.
	\end{pop}
	
	\begin{proof}
		Soit $X$ un MBL.
		$\esp \left[ (X_s - X_t)^4 \right] = (t - s)^2 \esp \left[ U^2 \right]$ où $U \sim \mathcal{N}(0,1)$ et on applique le théorème de Kolmogorov.
	\end{proof}
	
	TODO\\
	
	\begin{pop}
		Soit $B$ un MB.
		Alors $\limsup_{t \to \infty} \frac{B_t}{\sqrt{t}} \overset{\text{p.s.}}{=} +\infty$, $\liminf_{t \to \infty} \frac{B_t}{\sqrt{t}} \overset{\text{p.s.}}{=} -\infty$, $\lim_{t \to \infty} \frac{B_t}{t} = 0$, $\limsup_{t \searrow 0} \frac{B_t}{\sqrt{t}} \overset{\text{p.s.}}{=} +\infty$ et $\liminf_{t \searrow 0} \frac{B_t}{\sqrt{t}} \overset{\text{p.s.}}{=} -\infty$.
		De plus le processus donné par $Z_t = t B_{1/t}$ est un MBL.
	\end{pop}
	
	\begin{rem}
		On peut prouver des résultats plus fins, comme $\limsup_{t \to \infty} \frac{B_t}{\sqrt{t \log \log t}} \overset{\text{p.s.}}{=} 1$ ou $\liminf_{t \to \infty} \frac{B_t}{\sqrt{t \log \log t}} \overset{\text{p.s.}}{=} -1$.
	\end{rem}
	
	\begin{proof}
		Soit $R := \limsup_{t \to \infty} \frac{B_t}{\sqrt{t}}$.
		On a $\forall s > 0, R = \limsup_{t \to \infty} \frac{B_{t + s}}{\sqrt{t + s}} = \limsup_{t \to \infty} \frac{B_{t + s}}{\sqrt{t}} = \limsup_{t \to \infty} \frac{B_{t + s} - B_s}{\sqrt{t}}$.
		Par conséquent $R$ est indépendante de $\sigma(B_u, u \leq s)$ pour tout $s$.
		Donc $R$ est indépendante de la tribu $\sigma(B)$ engendrée par $B$.
		Comme $R$ est $\sigma(B)$-mesurable, $R$ est indépendant d'elle même : $\forall H \in \mathcal{B}(\R), \proba(R \in H) = P(R \in H)^2$.
		Donc $P(R \in H)$ vaut $0$ ou $1$.
		Donc $R = a$ avec proba $1$ où $a \in \intff{-\infty}{+\infty}$.
		
		Supposons $a < \infty$.
		Soit $b > a$ quelconque.
		Comme $R = a$ on peut vérifier que $\proba \left( \frac{B_t}{\sqrt{t}} > b \right) \underset{t \to \infty}{\longrightarrow} 0$.
		Mais par ailleurs $\frac{B_t}{\sqrt{t}} \sim \mathcal{N}(0,1)$, d'où une contradiction.
		Par conséquent $R = \infty$.
		La 2\up{e} et la 3\up{e} convergences se démontrent de la même façon.
		
		Pour prouver la 3\up{e} convergence et le résultat sur $Z_t = t B_{1/t}$.
		Nous avons que $Z_t$ est une gaussienne centrée.
		On peut prouver facilement que $\esp Z_t Z_s = s \wedge t$.
		$Z_t$ est continue sur $\intoo{0}{\infty}$ car $B_t$ est continue.
		Alors $\lim_{t \searrow 0} Z_t = \lim_{t \searrow 0} t B_{1/t} = \lim_{u \to \infty} \frac{B_u}{u} \overset{\text{p.s.}}{=} 0$.
		Donc $Z_t$ est un MBL dont presque toutes les trajectoires sont continues sur $\intfo{0}{\infty}$.
		Nous avons alors $\limsup_{t \searrow 0} \frac{B_t}{\sqrt{t}} = \limsup_{t \to \infty} \frac{Z_t}{\sqrt{t}} = +\infty$.
	\end{proof}
	
	\begin{cor}
		Avec probabilité 1 on a :
		\begin{enumerate}[(i)]
			\item Le MB passe une infinité de fois par chaque point de $\R$.
			\item Le MB n'est dérivable ni à droite pour tout $t \in \R_+$, ni à gauche pour tout $t \in \R_+^*$.
		\end{enumerate}
	\end{cor}
	
	\begin{proof}
		Pour \textit{(i)}, utiliser les convergences de la proposition précédente conjointement avec la continuité du MB.
		
		Pour \textit{(ii)}, prenons $t > 0$.
		Pour $s > 0$ nous avons $\frac{B_{t+s} - B_t}{s} = \frac{1}{\sqrt{s}} \cdot \frac{B_{t+s} - B_t}{\sqrt{s}}$, mais $Z_s := B_{t + s} - B_t$ est un MB.
		Comme $\limsup_{s \searrow 0} \frac{Z_s}{\sqrt{s}} \overset{\text{p.s.}}{=} +\infty$ on a le résultat.
	\end{proof}


\subsection{Mesurabilité du MB}

	On peut considérer un processus $X \colon \Omega \to E^{\mathbf{T}}$ où $\mathbf{T} = \R_+$ comme une application $\Omega \times \mathbf{T} \to E$ qui, à chaque couple $(\omega,t) \in \Omega \times \mathbf{T}$, associe $X_t(\omega)$.
	Si on adopte ce point de vue, on est amené à considérer la mesurabilité de $X$ par rapport à la tribu-produit $\mathcal{F} \otimes \mathcal{B}(\mathbf{T})$.
	
	\begin{defn}
		Un processus $X = (X_t, t \in \mathbf{T})$ à valeurs dans $E$ est dit \textbf{mesurable} si l'application $(\omega,t) \mapsto X_t(\omega)$ est mesurable de $(\Omega \times \mathbf{T}, \mathcal{F} \otimes \mathcal{B}(\mathbf{T})$ dans $(E,\mathcal{E})$.
	\end{defn}
	
	En présence de mesurabilité, les trajectoires $t \mapsto X_t(\omega)$ à $\omega$ fixé sont mesurables pour la tribu $\mathcal{B}(\mathbf{T})$.
	En particulier, le bruit blanc n'est pas mesurable en ce sens (bien qu'il soit mesurable au sens de Kolmogorov) car ses trajectoires sont trop irrégulières si $\nu$ n'est pas un Dirac.
	Aucune trajectoire de ce processus n'est borélienne.
	
	Quand le processus $X$ est mesurable, l'intégrale $\int_a^b \varphi(X_t(\omega)) \diff t$ a un sens pour toute fonction mesurable $\varphi$, et par Fubini $\esp \left[ \int_a^b \varphi(X_t(\omega)) \diff t \right] = \int_a^b \esp \varphi(X_t(\omega)) \diff t$ si $\int_a^b \esp \abs{\varphi(X_t(\omega))} \diff t < \infty$.
	
	\begin{note}
		Si $\forall \omega \in \Omega, t \mapsto X_t(\omega)$ est continue à droite (resp. à gauche), on dit que le processus est continu à droite (resp. à gauche).
	\end{note}
	
	\begin{pop}
		Si un processus $X$ est continu à gauche ou à droite, il est mesurable (par rapport à la tribu produit).
	\end{pop}
	
	\begin{proof}
		Supposons $X$ continu à gauche.
		Pour tout $n \in \N$, soit $X_n(t) := X \left( \frac{\lfloor nt \rfloor}{n} \right)$.
		Alors on peut vérifier que $X_n(t) \underset{n \to \infty}{\longrightarrow} X(t)$.
		Or $X_n(t)$ est toujours mesurable.
		Donc $X$ l'est par passage à la limite.
	\end{proof}
	
	\begin{cor}
		Le MB est mesurable.
	\end{cor}

	Notre but est maintenant de construire une intégrale de type $\int_0^t \varphi(s) \diff B_s$ où $B$ est un MB et où $\varphi$ est une fonction déterministe qui appartient à une classe appropriée.


\subsection{Rappels sur les fonctions à variations finies}

	Soit $\cont_0(\R_+)$ (resp. $\cont_0^+(\R_+)$) l'ensemble des fonctions continues (resp. continues croissantes) issues de zéro.
	Soit $\pi_t = \{ t_0,\ldots,t_n \}, 0 = t_0 < t_1 < \ldots < t_n = t$ une subdivision finie de l'intervalle $\intff{0}{t}$.
	
	\begin{defn}
		La \textbf{variation approchée} d'une fonction $f \in \cont_0(\R_+)$ sur la subdivision $\pi_t$ est $V_1(f,\pi_t,t) := \sum_{i = 1}^n \abs{f(t_i) - f(t_{i - 1})}$.
		La fonction $f$ est dit à \textbf{variation finie} si $\forall t, V_1(f,t) := \sup_{\pi_t} V_1(f,\pi_t,t)$ est finie.
	\end{defn}
	
	\begin{pop}
		Si $f \in \cont_0(\R_+)$ est à variations finies, alors elle s'écrit d'une manière unique $f = f_+ - f_-$ où :
		\begin{enumerate}[(i)]
			\item $f_+ \in \cont_0^+(\R_+)$, $f_- \in \cont_0^+(\R_+)$,
			\item $\forall f_+',f_-' \in \cont_0^+(\R_+)$ telles que $f = f_+' - f_-'$ on a $f_+' - f_+ \in \cont_0^+(\R_+)$ et $f_-' - f_- \in \cont_0^+(\R_+)$.
		\end{enumerate}
	\end{pop}
	
	Nous savons que si $g \in \cont_0^+(\R_+)$ alors la fonction d'ensemble $\mu(\intof{a}{b}) := g(b) - g(a)$ pour tout $a \leq b$ est une mesure (de Radon) positive sur $\R_+$.
	Soit $\diff f_+$ et $\diff f_-$ les mesures associées à $f_+$ et $f_-$ de cette façon.
	Pour toute fonction borélienne $\varphi$ sur $\R_+$ qui satisfait $\int \abs{\varphi} \diff f_+ < \infty$ et $\int \abs{\varphi} \diff f_- < \infty$, nous écrirons $\int \varphi \diff f := \int \varphi \diff f_+ - \int \varphi \diff f_- = \int \varphi (\diff f_+ - \diff f_-)$.
	C'est l'intégrale de Lebesgue-Stieltjes par rapport à une fonction à variation finie.


\subsection{Variation quadratique d'un MB}

	\begin{defn}
		La \textbf{variation quadratique approchée} d'une fonction $f \in \cont_0(\R_+)$ sur la subdivision $\pi_t$ est $V_2(f,\pi_t,t) := \sum_{i = 1}^n (f(t_i) - f(t_{i-1}))^2$.
	\end{defn}
	
	\begin{pop}
		Si $f$ est à variation finie alors $V_2(f,\pi_t,t) \underset{\abs{\pi_t} \to 0}{\longrightarrow} 0$ où $\abs{\pi_t} := \max_i \abs{t_i - t_{i-1}}$.
	\end{pop}
	
	\begin{proof}
		Comme $f$ est continue sur $\intff{0}{t}$, elle est uniformément continue, i.e $\forall \varepsilon > 0, \exists \eta > 0, \forall t_1,t_2 \in \intff{0}{t}, \abs{t_1 - t_2} < \eta \implies \abs{f(t_1) - f(t_2)} < \varepsilon$.
		Par conséquent, si $\abs{\pi_t} < \eta$,
		$$V_2(f,\pi_t,t) \leq \varepsilon \sum_{i = 1}^n \abs{f(t_i) - f(t_{i-1})} = \varepsilon V_1(f,\pi_t,t) \leq \varepsilon V_1(f,t)\ .$$
		Comme $\varepsilon$ est quelconque, on a le résultat.
	\end{proof}
	
	\begin{thm}
		Sur tout intervalle $\intff{0}{1}$ où $t  0$, presque toutes les trajectoires d'un MB sont à variation infinie.
	\end{thm}
	
	\begin{proof}
		On montre $\forall t > 0, V_2(B,\pi_t,t) \underset{\abs{\pi_t} \to 0}{\overset{\mathcal{L}^2}{\longrightarrow}} t\quad  (*)$.
		En effet, soit $Y_n := \sum_{i = 1}^n \left( B_{t_i} - B_{t_{i-1}} \right)^2$.
		En écrivant $B_{t_i} - B_{t_{i-1}} = \sqrt{t_i - t_{i-1}} Z_i$ où $Z_i \sim \mathcal{N}(0,1)$ et où les $Z_i$ sont indépendantes, on a $\esp Y_n = t$ et
		$$\Var(Y_n) = \sum_{i = 1}^n \Var \left( \left( B_{t_i} - B_{t_{i-1}} \right)^2 \right)
			= \Var \left( Z_1^2 \right) \sum_{i = 1}^n (t_i - t_{i - 1})^2
			\leq \Var \left( Z_1^2 \right) \abs{\pi_t} \sum_{i = 1}^n (t_i - t_{i - 1})
			= \Var \left( Z_1^2 \right) \abs{\pi_t} t$$
		qui tend vers $0$ avec $\abs{\pi_t}$, d'où $(*)$.
		
		Considérons une suite de subdivisions $\pi_t^n$ telle que $\abs{\pi_t^n} \underset{n \to \infty}{\longrightarrow} 0$.
		Les v.a $Y_n$ associées tendent dans $\mathcal{L}^2$, donc en probabilité, vers $t$.
		Par conséquent il existe une sous-suite $\left( \pi_t^{\varphi(n)} \right)$ telle que $V_2(B,\pi_t^{\varphi(n)},t) \underset{n \to \infty}{\overset{\text{p.s.}}{\longrightarrow}} t > 0$.
		La proposition précédente nous dit que sur cet ensemble de proba 1, $B_t$ n'est pas à variation finie.
	\end{proof}

	Conclusion : on ne peut pas utiliser la théorie de Lebesgue pour construire des intégrales du type $\int_0^t \varphi(s) \diff B_s$.


\subsection{L'intégrale de Wiener}

	L'intégrale de Wiener est définie sur l'espace de Hillbert $L^2(\R_+)$ des fonctions de carré intégrable par rapport à la mesure de Lebesgue sur $\R_+$.
	C'est une isométrie entre cet espace et l'espace de Hilbert $\mathcal{L}^2$ des variables aléatoires qui ont un 2\up{nd} moment fini.
	Rappelons que ces deux espaces sont munis des normes respectives $\norme{\varphi}_{L^2(\R_+)} = \left( \int_{\R_+} \varphi(s)^2 \diff s \right)^{\frac{1}{2}}$ et $\norme{X}_{\mathcal{L}^2} = \left( \esp \left[ X^2 \right] \right)^{\frac{1}{2}}$.
	
	\begin{thm}[\textbf{Intégrale de Wiener}]
		Soit $B$ un MB.
		Il existe un opérateur linéaire isométrique $I \colon L^2(\R_+) \to \mathcal{L}^2$, unique à une classe d'équivalence près pour l'égalité presque partout, et qui satisfait $I(\indic_{\intof{s}{t}}) = B_t - B_s$ pour tous $0 \leq s \leq t$.
		Par ailleurs $\esp[I(\varphi)] = 0$ pour tout $\varphi \in L^2(\R_+)$.
		Nous écrivons $I(\varphi) = \int_{\R_+} \varphi(t) \diff B_t$.
	\end{thm}
	
	\begin{rem}
		Dire que $I$ est une isométrie revient à dire $\forall \varphi \in L^2(\R_+), \esp \left[ I(\varphi)^2 \right] = \int \varphi^2(x) \diff x$.
	\end{rem}
	
	\begin{proof}
		Dans un premier temps, nous construisons $I$ sur l'ensemble $\mathcal{E}$ des fonctions en escalier, i.e de la forme $\varphi = \sum_{i = 1}^n a_i \indic_{\intof{t_{i-1}}{t_i}}$ où $0 \leq t_0 < t_1 < \ldots < t_n$.
		Par linéarité $I(\varphi) = \sum_{i = 1}^n a_i \left( B_{t_{i-1}} - B_{t_i} \right)$.
		Aussi nous avons sur $\mathcal{E}$,
		$$\norme{I(\varphi)}_{\mathcal{L}^2}^2
			= \esp \left[ \left( \sum_{i = 1}^n a_i (B_{t_{i-1}} - B_{t_i}) \right)^2 \right]
			= \sum_{i = 1}^n a_i^2 (t_i - t_{i-1})
			= \norme{\varphi}_{L^2(\R_+)}^2\ .$$
		Comme $\mathcal{E}$ est dense dans $L^2(\R_+)$, l'opérateur $I$ se prolonge d'une manière unique en une isométrie sur $L^2(\R_+)$.
		Il reste à prouver que $\esp[I(\varphi)] = 0$ pour tout $\varphi \in L^2(\R_+)$.
		Le résultat est évident sur $\mathcal{E}$.
		
		Soit $(\varphi_n)_n$ une suite de $\mathcal{E}$ qui tend vers $\varphi$ dans $L^2(\R_+)$.
		On a
		$$\abs{\esp[I(\varphi)]}
			= \abs{\esp[I(\varphi - \varphi_n)]}
			\leq \esp \abs{I(\varphi - \varphi_n)}
			\leq \norme{I(\varphi - \varphi_n)}_{\mathcal{L}^2}
			= \norme{\varphi - \varphi_n}_{L^2(\R_+)}$$
		en utilisant $\esp \abs{X} \leq \left( \esp[X^2] \right)^{1/2}$ et $I$ est une isométrie.
		
		Comme $\abs{\varphi - \varphi_n}_{L^2(\R_+)} \longrightarrow 0$ nous avons le résultat.
	\end{proof}
	
	\begin{pop}
		On a $\forall \varphi \in L^2(\R_+), I(\varphi) \sim \mathcal{N} \left( 0, \norme{\varphi}_{L^2(\R_+)}^2 \right)$.
	\end{pop}
	
	\begin{proof}
		On sait que $\esp[I(\varphi)] = 0$ et $\esp \left[ I(\varphi)^2 \right] = \norme{\varphi}^2_{L^2(\R_+)}$.
		Reste à établir la gaussianité.
		Pour ceci il suffit d'approximer $\varphi$ par une suite de fonctions dans $\mathcal{E}$ (dont les intégrales de Wiener sont par construction des gaussiennes) et de passer à la limite en utilisant un résultat du chapitre sur la loi gaussienne.
	\end{proof}