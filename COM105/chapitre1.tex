\begin{defn}[Le modèle de transmission]
	$$\overset{D_1,\ldots,D_k}{\longrightarrow}
		\text{Émetteur } f
	\overset{X_1,\ldots,X_n}{\longrightarrow}
		\text{Canal}
	\overset{Y_1,\ldots,Y_n}{\longrightarrow}
		\text{Récepteur } g
	\overset{\hat{D}_1,\ldots,\hat{D}_k}{\longrightarrow}$$
\end{defn}

\begin{voc}
	\begin{itemize}
		\item[\textbullet] Bits d'information : $\mathbf{D} = (D_1,\ldots,D_k)$, représentent les données à transmettre, supposés iid $\mathcal{B} \left( \frac{1}{2} \right) $.
		\item[\textbullet] Transmission en bloc : les $k$ bits d'information sont envoyés sur un bloc de $n$ utilisations du canal.
		\item[\textbullet] Émetteur : associe $\mathbf{X} = (X_1,\ldots,X_kn)$ à $\mathbf{D} = (D_1,\ldots,D_k)$, supposé déterministe et avec $f$ injective.
		\item[\textbullet] Récepteur : associe $\mathbf{\hat{D}} = \left( \hat{D}_1,\ldots,\hat{D}_k \right)$ à $\mathbf{Y} = (Y_1,\ldots,Y_kn)$, supposé déterministe.
		\item[\textbullet] Erreur : cas où $\left( \hat{D}_1,\ldots,\hat{D}_k \right) \neq (D_1,\ldots,D_k)$.
	\end{itemize}
\end{voc}


\subsection{Les canaux}

	\begin{defn}[Canaux discrets sans mémoire (\textbf{DMC})]
		Un DMC est complètement caractérisé par le triplet $\left( \mathcal{X}, \mathcal{Y}, \proba_{X \mid Y}(\cdot \mid \cdot) \right)$ où
		\begin{itemize}
			\item $\mathcal{X}$ est un alphabet fini contenant toutes les valeurs possibles à l'entrée du DMC,
			\item $\mathcal{Y}$ est un alphabet fini contenant toutes les valeurs possibles à la sortie du DMC,
			\item $\proba_{X \mid Y}(\cdot \mid \cdot)$ est une loi de probabilité conditionnelle, dite loi de transition, décrivant comment une sortie $Y_t$ est obtenue à partir d'une entrée $x_t$.
		\end{itemize}
	\end{defn}

	\begin{defn}[Canal binaire symétrique (\textbf{BSC})]
		On a $\mathcal{X} = \mathcal{Y} = \{ 0,1 \}$ et $\forall t \in \intiff{1}{n}$, $\proba(Y_t = x_t) = p \in \inff{0}{1}$ et $\proba(Y_t \neq x_t) = 1 - p$.
		On peut toujours supposer $p < \frac{1}{2}$.
	\end{defn}

	\begin{center}
	\begin{tikzpicture}
	\matrix(m)[matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
	{
		0 & 0 \\
		1 & 1\\};
	\path[-stealth]
		(m-1-1) edge node [above] {$1 - p$} (m-1-2)
				edge node [above] {$p$} (m-2-2)
		(m-2-1) edge node [below] {$p$} (m-1-2)
				edge node [below] {$1 - p$} (m-2-2);
	\end{tikzpicture}
	\end{center}

	\begin{defn}[Canal binaire à effacement (\textbf{BEC})]
		On a $\mathcal{X} = \{ 0,1 \}$ et $\mathcal{Y} = \{ 0,1,\Delta \}$ où $\Delta$ représente un effacement.
		Pour tout $t \in \intiff{1}{n}$, $\proba(Y_t = x_t) = \epsilon \in \inff{0}{1}$ et $\proba(Y_t = \Delta) = 1 - \epsilon$.
	\end{defn}

	\begin{center}
	\begin{tikzpicture}
	\matrix(m)[matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
	{
		0 & 0 \\
		  & \Delta\\
		1 & 1\\};
	\path[-stealth]
		(m-1-1) edge node [above] {$\epsilon$} (m-1-2)
				edge node [left] {$1 - \epsilon$} (m-2-2)
		(m-3-1) edge node [left] {$1 - \epsilon$} (m-2-2)
				edge node [below] {$\epsilon$} (m-3-2);
	\end{tikzpicture}
	\end{center}

\subsection{Codage par des codes en bloc}

	\begin{defn}
		Un \textbf{code en bloc} $\mathcal{C}$ de longueur $n$ sur un alphabet $\mathcal{X}$ est un sous-ensemble de $\mathcal{X}^n$, c'est l'ensemble d'arrivée de $f$.
		Les éléments de $\mathcal{C}$ sont appelés les mots de code de $\mathcal{C}$.
	\end{defn}

	Principe : $\underset{k\ \text{bits}}{[\quad m\quad ]} \longrightarrow \underset{n\ \text{bits}}{[\ c \in \mathcal{C}\ ]}$ avec $n > k$.

	\begin{defn}
		Le \textbf{rendement} (binaire) d'un code en bloc $\mathcal{C}$ de longueur $n$, aussi appelé taux de codage, est $R = \frac{\log_2(\abs{\mathcal{C}})}{n}$ où $\abs{\mathcal{C}}$ est le nombre de mots du code $\abs{\mathcal{C}}$.
	\end{defn}


\subsection{Distances}

	\begin{defn}
		Poids de Hamming pour $\mathbf{x} = (x_1,\ldots,x_n)$ : $w_H(\mathbf{x}) = \card \left( \left\{ x_i \neq 0 \right\} \right)$.
	\end{defn}

	\begin{defn}
		La \textbf{distance de Hamming} entre deux mots $\mathbf{x}$ et $\mathbf{\hat{x}}$ est donnée par $d_H(\mathbf{x},\mathbf{\hat{x}}) = w_H(\mathbf{x} - \mathbf{\hat{x}})$.
	\end{defn}

	\begin{pop}
		La distance de Hamming est bien une distance (symétrie, positivité et inégalité triangulaire).
	\end{pop}

	\begin{defn}
		La \textbf{distance minimale} du code en bloc $\mathcal{C}$ est
		$d_{\min}(\mathcal{C}) =
			\min_{ \mathbf{c} \neq \mathbf{\hat{c}} }
			d_H(\mathbf{c},\mathbf{\hat{c}})$.
	\end{defn}


\subsection{Décodage}

	On décompose la fonction de décodage en deux étapes : $g = g_2 \circ g_1$, $g_1$ trouve pour toute observation $Y$ le mot de code $\mathbf{\hat{c}} \in \mathcal{C}$ qui paraît le plus probable et $g_2 = f^{-1}$ produit la suite des bits détectés $\hat{D}_1,\ldots,\hat{D}_k$ qui est associée à $\hat{c}$.

	\begin{defn}
		Soit $g_1$ fixée. La région de décision associée à $c \in \mathcal{C}$ est $\Omega_{\mathbf{c}} := g_1^{-1}(c)$.
		Ces régions forment une partition de $\mathcal{Y}^n$.
	\end{defn}

	On a $P_e := \proba(\mathbf{\hat{C}} \neq \mathbf{C})$ la probabilité d'erreur et $P_c := \proba(\mathbf{\hat{C}} = \mathbf{C})$ la probabilité de succès.

	\begin{pop}[Optimalité de la règle de maximum vraisemblance, \textbf{maximum likelihood}]
		Si les mots de code sont équiprobables en entrée, minimiser $P_e$ revient à choisir $\hat{c} \in \argmax_{x \in \mathcal{C}} p(Y \mid x)$.
	\end{pop}

	\begin{pop}[\textbf{Règle du voisin le plus proche}]
		Dans le cas d'un BSC on a
		$\forall p \in \intff{0}{\frac{1}{2}}, \proba(\mathbf{Y} = \mathbf{y} \mid \mathbf{C} = \mathbf{c}) = (1 - p)^n \left( \frac{p}{1 - p} \right)^{d_H(\mathbf{y},\mathbf{c})}$
		donc minimiser $P_e$ revient à trouver le mot de code $\mathbf{c} \in \mathcal{C}$ qui minimise $d_H(\mathbf{y},\mathbf{c})$.
	\end{pop}

	\begin{voc}
		On dit qu'un code en bloc $\mathcal{C}$ corrige $t$ erreurs si il existe un décodeur qui permet de corriger toutes les configurations d'erreurs dont le nombre est inférieur ou égal à $t$.
	\end{voc}

	\begin{pop}[\textbf{Capacité de correction} d'un code]
		Le décodeur décide toujours du bon mot $\mathbf{\hat{c}} = \mathbf{c}$ lorsque $2 d_H(\mathbf{c},\mathbf{y}) < d_{\min}$.
		Donc le code peut corriger $t = \left\lfloor \frac{d_{\min} - 1}{2} \right\rfloor$ erreurs.
	\end{pop}

	Lorsque l'on fait de la détection d'erreur, on a $g_1 \colon \mathcal{Y}^n \to \mathcal{C} \cup \Delta$.
	La question est alors : est-ce que le mot reçu est bien égal au mot envoyé ?
	Dans le cas où $d_H(\mathbf{c},\mathbf{y}) = l \geq 1$ et le décodeur produit $\Delta$, on dit que le décodeur a détecté $l$ erreurs.

	\begin{voc}
		Dans le cas d'un BEC, on dit qu'un code en bloc $\mathcal{C}$ détecte $t$ erreurs si il existe un décodeur qui permet de corriger toutes les configurations d'erreurs dont le nombre est inférieur ou égal à $t$.
	\end{voc}

	\begin{pop}[\textbf{Capacité de détection} d'un code]
		Un code en bloc est capable de détecter $t' = d_{\min} - 1$ erreurs.
	\end{pop}
	Il suffit pour cette détection de déclarer $\Delta$ dès que $\mathbf{y} \not\in \mathcal{C}$.