\subsection{Statistical modeling}

	\begin{defn}
		Let $(\Omega,\mathcal{F})$ be a measurable space and $\mathcal{P}$ a collection of probabilities on this space.
		Let $X$ be a measurable function from $(\Omega,\mathcal{F})$ to the observation space $(\mathsf{X},\mathcal{X})$.
		We say that $\mathcal{P}$ is a \textbf{statistical model} for the observation variable $X$ and denote $\mathcal{P}^X = \left( P^X \right)_{P \in \mathcal{P}}$ the corresponding collection of probability distributions.
	\end{defn}

	It is usual in statistics to consider $\Omega = \mathsf{X}$, $\mathcal{F} = \mathcal{X}$ and $X(\omega) = \omega$, in which case $\forall P \in \mathcal{P}, P = P^X$.

	\begin{defn}
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$ and $\mathcal{P}$ be a statistical model for $X$.
		We say that $\mathcal{P}$ is a $\nu$-dominated model for $X$, or that $\mathcal{P}^X$ is $\nu$-dominated, if $\forall P \in \mathcal{P}, P^X \ll \nu$.
	\end{defn}

	\begin{lem}
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$.
		Consider a $\nu$-dominated model $\mathcal{P}$ for the variable $X$.
		Then there exists a countable collection $(P_n)_{n \geq 1}$ in $\mathcal{P}$ such that $\mathcal{P}^X$ is also dominated by $\mu = \sum_{n \geq 1} 2^{-n} P_n^X$.
	\end{lem}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for the observation variable $X$.
		We say that $\mathcal{P}$ is a \textbf{parametric model} for $X$ if there exists a finite dimensional set $\Theta$ such that $\mathcal{P} = (P_\theta)_{\theta \in \Theta}$.
	\end{defn}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for $X$.
		Any finite dimensional quantity $t(P^X)$ only depending on $P^X$ as $P \in \mathcal{P}$ is called an \textbf{identifiable parameter}.
	\end{defn}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for $X$.
		A \textbf{statistic} in this context is any random variable $T$ valued in $\left(\R^d, \mathcal{B}\left( \R^d \right)\right)$ with $d \geq 1$, defined by $T = g(X)$ where $g$ is a Borel function not depending on $P \in \mathcal{P}$.
	\end{defn}

	If a statistic is used as a guess for a parameter $t(P) \in \R^d$, it is called an \textbf{estimator} of $t(P)$.
	In this case, the \textbf{bias} of $T$ for estimating $t(P)$ is defined as $\Bias(T,P) = \int T \diff P - t(P)$ whenever $\int \abs{T} \diff P < \infty$.
	We say that $T$ is an \emph{unbiased} estimator of $t(P)$ if $\forall P \in \mathcal{P}, \int T \diff P = t(P)$.
	The \textbf{quadratic risk} or \textbf{mean squared error} (in the case $d = 1$) is defined by $\MSE(T,P) = \int (T - t(P))^2 \diff P = \Var(T) + \Bias(T,P)^2$.

	\begin{defn}
		Let $T$ be a statistic valued in $(\R^d, \mathcal{\R^D})$ with $d \geq 1$.
		We say that $T$ is a \textbf{sufficient statitstic} for the model $\mathcal{P}$ if, for all $P \in \mathcal{P}$, the conditional distribution of $X$ given $T$ does not depend on $P$, that is, there exists a probability kernel $Q \subset \R^d \times \mathcal{X}$ such that, for all $P \in \mathcal{P}$, $Q$ is a regular version of $P^{X \mid T}$.
	\end{defn}

	\begin{lem}
		Let $S$ be a sufficient statistic associated to the Markov kernel $Q$ and let $T = g(X)$ be an unbiased estimator of the parameter $t(P)$ (both real valued).
		Define $T^R = \int g(x) Q(S,\diff x)$.
		Then $T^R$ is an unbiased estimator of the parameter $t$ and its variance is smaller than that of $T$.
		As a consequence we have, $\forall P \in \mathcal{P}, \MSE \left( T^R, P \right) \leq \MSE (T,P)$.
	\end{lem}

	\begin{thm}[Fisher Factorization theorem]
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$.
		Consider a $\nu$-dominated model $\mathcal{P}$ for $X$ and let $S = g(X)$ be a $d$-dimensional statistic.
		Then $S$ is a sufficient statistic for the model $\mathcal{P}$ if and only if there exists a non-negative Borel function $h$ on $\mathsf{X}$ such that $\forall P \in \mathcal{P}$, there exists a Borel function $f_P \colon \R^d \to \R_+$ such that $\frac{\diff P^X}{\diff \nu} = h \cdot f_P \circ g$.
	\end{thm}
	
	\begin{defn}
		Consider a $\nu$-dominated model $\mathcal{P}$ for $X$.
		For all $P \in \mathcal{P}$, let us denote by $f_P$ the density of $P^X$ with respect to $\nu$.
		The \textbf{likelihood function} is defined as $P \mapsto f_P \circ X$ on $P \in \mathcal{P}$.
	\end{defn}
	
	Then, $f_{P_1}(X) \geq f_{P_2}(X)$ is an indication that $\KL \left( P_*^X \| P_1^X \right) \leq \KL \left( P_*^X \| P_2^X \right)$ where $P_*$ is the true distribution of $X$.

	\begin{rem}
		Interestingly, we note that if one has a sufficient statistic $S = g(X)$, by the Fisher Factorization theorem, to compare $f_{P_1}(X)$ and $f_{P_2}(X)$, we only need to observe $S$.
	\end{rem}

	With a parametric model we define the likelihood function directly on $\Theta$, $\theta \mapsto f_\theta \circ X$ where $f_\theta$ denotes the density of $P_\theta$ with respect to $\nu$.
	
	\begin{defn}
		A statistic $\hat{\theta}_n$ valued in $\Theta$ such that $f_{\hat{\theta}_n} \circ X = \max_{\theta \in \Theta} f_\theta \circ X$ is called a \textbf{maximum likelihood estimator (MLE)}.
	\end{defn}
