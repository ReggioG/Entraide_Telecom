\subsection{Statistical modeling}

	\begin{defn}
		Let $(\Omega,\mathcal{F})$ be a measurable space and $\mathcal{P}$ a collection of probabilities on this space.
		Let $X$ be a measurable function from $(\Omega,\mathcal{F})$ to the observation space $(\mathsf{X},\mathcal{X})$.
		We say that $\mathcal{P}$ is a \textbf{statistical model} for the observation variable $X$ and denote $\mathcal{P}^X = \left( P^X \right)_{P \in \mathcal{P}}$ the corresponding collection of probability distributions.
	\end{defn}

	It is usual in statistics to consider $\Omega = \mathsf{X}$, $\mathcal{F} = \mathcal{X}$ and $X(\omega) = \omega$, in which case $\forall P \in \mathcal{P}, P = P^X$.

	\begin{defn}
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$ and $\mathcal{P}$ be a statistical model for $X$.
		We say that $\mathcal{P}$ is a $\nu$-dominated model for $X$, or that $\mathcal{P}^X$ is $\nu$-dominated, if $\forall P \in \mathcal{P}, P^X \ll \nu$.
	\end{defn}

	\begin{lem}[Halmos and Savage]
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$.
		Consider a $\nu$-dominated model $\mathcal{P}$ for the variable $X$.
		Then there exists a countable collection $(P_n)_{n \geq 1}$ in $\mathcal{P}$ such that $\mathcal{P}^X$ is also dominated by $\mu = \sum_{n \geq 1} 2^{-n} P_n^X$.
	\end{lem}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for the observation variable $X$.
		We say that $\mathcal{P}$ is a \textbf{parametric model} for $X$ if there exists a finite dimensional set $\Theta$ such that $\mathcal{P} = (P_\theta)_{\theta \in \Theta}$.
	\end{defn}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for $X$.
		Any finite dimensional quantity $t(P^X)$ only depending on $P^X$ as $P \in \mathcal{P}$ is called an \textbf{identifiable parameter}.
	\end{defn}

	\begin{defn}
		Let $\mathcal{P}$ be a statistical model for $X$.
		A \textbf{statistic} in this context is any random variable $T$ valued in $\left(\R^d, \mathcal{B}\left( \R^d \right)\right)$ with $d \geq 1$, defined by $T = g(X)$ where $g$ is a Borel function not depending on $P \in \mathcal{P}$.
	\end{defn}

	If a statistic is used as a guess for a parameter $t(P) \in \R^d$, it is called an \textbf{estimator} of $t(P)$.
	In this case, the \textbf{bias} of $T$ for estimating $t(P)$ is defined as $\Bias(T,P) = \int T \diff P - t(P)$ whenever $\int \abs{T} \diff P < \infty$.
	We say that $T$ is an \emph{unbiased} estimator of $t(P)$ if $\forall P \in \mathcal{P}, \int T \diff P = t(P)$.
	The \textbf{quadratic risk} or \textbf{mean squared error} (in the case $d = 1$) is defined by $\MSE(T,P) = \int (T - t(P))^2 \diff P = \Var(T) + \Bias(T,P)^2$.

	\begin{defn}
		Let $T$ be a statistic valued in $(\R^d, \mathcal{\R^D})$ with $d \geq 1$.
		We say that $T$ is a \textbf{sufficient statistic} for the model $\mathcal{P}$ if, for all $P \in \mathcal{P}$, the conditional distribution of $X$ given $T$ does not depend on $P$, that is, there exists a probability kernel $Q \subset \R^d \times \mathcal{X}$ such that, for all $P \in \mathcal{P}$, $Q$ is a regular version of $P^{X \mid T}$.
	\end{defn}

	\begin{lem}
		Let $S$ be a sufficient statistic associated to the Markov kernel $Q$ and let $T = g(X)$ be an unbiased estimator of the parameter $t(P)$ (both real valued).
		Define $T^R = \int g(x) Q(S,\diff x)$.
		Then $T^R$ is an unbiased estimator of the parameter $t$ and its variance is smaller than that of $T$.
		As a consequence we have, $\forall P \in \mathcal{P}, \MSE \left( T^R, P \right) \leq \MSE (T,P)$.
	\end{lem}

	\begin{thm}[Fisher Factorization theorem]
		Let $\nu \in \mes_+(\mathsf{X},\mathcal{X})$.
		Consider a $\nu$-dominated model $\mathcal{P}$ for $X$ and let $S = g(X)$ be a $d$-dimensional statistic.
		Then $S$ is a sufficient statistic for the model $\mathcal{P}$ if and only if there exists a non-negative Borel function $h$ on $\mathsf{X}$ such that $\forall P \in \mathcal{P}$, there exists a Borel function $f_P \colon \R^d \to \R_+$ such that $\frac{\diff P^X}{\diff \nu} = h \cdot f_P \circ g$.
	\end{thm}
	
	\begin{defn}
		Consider a $\nu$-dominated model $\mathcal{P}$ for $X$.
		For all $P \in \mathcal{P}$, let us denote by $f_P$ the density of $P^X$ with respect to $\nu$.
		The \textbf{likelihood function} is defined as $P \mapsto f_P \circ X$ on $P \in \mathcal{P}$.
	\end{defn}
	
	Then, $f_{P_1}(X) \geq f_{P_2}(X)$ is an indication that $\KL \left( P_*^X \| P_1^X \right) \leq \KL \left( P_*^X \| P_2^X \right)$ where $P_*$ is the true distribution of $X$.

	\begin{rem}
		Interestingly, we note that if one has a sufficient statistic $S = g(X)$, by the Fisher Factorization theorem, to compare $f_{P_1}(X)$ and $f_{P_2}(X)$, we only need to observe $S$.
	\end{rem}

	With a parametric model we define the likelihood function directly on $\Theta$, $\theta \mapsto f_\theta \circ X$ where $f_\theta$ denotes the density of $P_\theta$ with respect to $\nu$.
	
	\begin{defn}
		A statistic $\hat{\theta}_n$ valued in $\Theta$ such that $f_{\hat{\theta}_n} \circ X = \max_{\theta \in \Theta} f_\theta \circ X$ is called a \textbf{maximum likelihood estimator (MLE)}.
	\end{defn}

\subsection{Statistical testing}

	We define two hypothesis, respectively called the \emph{null hypothesis} and the \emph{alternative hypothesis}.
	\begin{itemize}
		\item[\textbullet] $(\mathbf{H}_0)$ the observation variable $X$ has distribution $P^X$ with $P \in \mathcal{P}_0$,
		\item[\textbullet] $(\mathbf{H}_1)$ $X$ has distribution $P^X$ with $P \in \mathcal{P}_1$,
	\end{itemize}
	with $\{ \mathcal{P}_0, \mathcal{P}_1 \}$ a partition of a statistical model $\mathcal{P}$.
	$(\mathbf{H}_i)$ is simple if $\mathcal{P}_i$ reduces to one point.
	
	\begin{defn}
		A \textbf{statistical test} is a statistic $\delta$ with values in $\{ 0, 1 \}$.
		If $\delta = 0$ we say that we accept $(\mathbf{H}_0)$.
		Otherwise we reject it.
	\end{defn}
	
	To evaluate the performance of a test $\delta$, two type of risks are considered :
	\begin{itemize}
		\item[\textbullet] The \emph{first type} risk is defined as $P \mapsto P(\delta = 1)$ as $P \in \mathcal{P}_0$.
		\item[\textbullet] The \emph{second type} risk is defined as $P \mapsto P(\delta = 0)$ as $P \in \mathcal{P}_1$.
	\end{itemize}
	We call \emph{power} of $\delta$ the application $P \mapsto P(\delta = 1)$ as $P \in \mathcal{P}_1$.

	\begin{defn}
		Let $\alpha \in \intff{0}{1}$.
		We say that a test $\delta$ is of level $\alpha$ if $\sup_{P \in \mathcal{P}_0} P(\delta = 1) \leq \alpha$.
		We say that $\delta$ is uniformly more powerful then $\delta'$ for level $\alpha$ if both are of level $\alpha$ and $\forall P \in \mathcal{P}_1, P(\delta = 1) \geq P(\delta' = 1)$.
	\end{defn}


\subsection{Simple hypotheses}

	We consider $\mathcal{P}_0 = \{ P_0 \}$ and $\mathcal{P}_1 = \{ P_1 \}$, with $f_0$ and $f_1$ the densities of $P_0^X$ and $P_1^X$ with respect to a common dominating measure.
	
	\begin{defn}
		The statistic $T = \frac{f_1(X)}{f_0(X)}$ is called the \textbf{likelihood ratio statistic}.
		Let $t \in \intff{0}{\infty}$.
		The test defined by
		$\delta = \left\{ \begin{array}{ll}
			1 & \text{if}\ T \geq t \\
			0 & \text{otherwise}
			\end{array}\right. $
		is called the \textbf{likelihood ratio test} with threshold $t$.
	\end{defn}
	
	\begin{thm}
		Denote by $T$ the likelihood ratio corresponding to $\mathcal{P}_0$ and $\mathcal{P}_1$.
		Let $t \in \intff{0}{\infty}$ and set $\alpha_t = P_0(T \geq t)$.
		Then the likelihood ratio test with threshold $t$ is uniformly more powerful than any other test $\delta'$ for the level $\alpha_t$.
		Moreover, if $\delta'$ is of level $\alpha_t$ and as powerful as $\delta$, then they coÃ¯ncide on the set $\{ T \neq t \}$ $P_i$-a.s. for $i \in \{ 0, 1 \}$.
	\end{thm}


%\subsection{EM algorithm}

\subsection{Fisher information matrix}

	We consider a parametric $\nu$-dominated model $\mathcal{P} = (P_\theta)_{\theta \in \Theta}$ for the observation variable $X$ valued in $(\mathsf{X},\mathcal{X})$, and denote by $f_\theta$ the density of $P_\theta$ with respect to $\nu$.
	We assume that $\Theta$ is an open subset of $\R^n$ and denote by $\norm{f} := \left( \int_{\mathsf{X}} \abs{f(x)}^2 \nu(\diff x) \right)^{\frac{1}{2}}$ the norm of the Hilbert space $L^2(\mathsf{X},\mathcal{X},\nu)$.
	Observe that $\forall \theta \in \Theta, \xi_\theta = \sqrt{f_\theta} \in L^2(\mathsf{X},\mathcal{X},\nu)$.

	\begin{defn}
		We say that $\mathcal{P}$ is \textbf{Hellinger differentiable} at $\theta$ if $\theta' \mapsto \xi_\theta$ defined from $\Theta \to L^2(\mathsf{X},\mathcal{X},\nu)$ admits a derivative at $\theta$ : $\exists ! \Dot{\xi}_\theta \in (L^2(\mathsf{X},\mathcal{X},\nu))^d, \lim_{\theta' \to \theta} \frac{1}{\abs{\theta' - \theta}} \norm{\xi_{\theta'} - \xi_{\theta} - \Dot{\xi}_\theta^T(\theta' - \theta)} = 0$.
	\end{defn}

	\begin{lem}
		Let $\theta \in \Theta$ and $V \subset \Theta$ be a neighborhood of $\theta$.
		Suppose that for $\nu$-a.e. $x$ and all $\theta' \in V$, we can write $\xi_{\theta'}(x) = \xi_{\theta}(x) + \int_{t = 0}^1 g_{t \theta' + (1-t) \theta}^T(x) (\theta' - \theta) \diff t$, where, for all $x \in \mathsf{X}$, $g$ satisfies one of the following assertions,
		\begin{enumerate}[(i)]
			\item we have $\lim_{\epsilon \downarrow 0} \norm{\sup_{\abs{\theta' - \theta} \leq \epsilon} \abs{g_{\theta'} - g_\theta}} = 0$,
			\item for $\nu$-a.e. $x$, $\theta' \mapsto g_{\theta'}(x)$ is continuous and $\exists \epsilon > 0, \norm{\sup_{\abs{\theta' - \theta} \leq \epsilon} \abs{g_{\theta'}}} < \infty$.
		\end{enumerate}
		Then $\mathcal{P}$ is Hellinger differentiable at $\theta$ with derivative $g_\theta$.
	\end{lem}
	
	The derivarive of $\theta \mapsto \ln f_\theta(X)$ is  called the score function.
	
	\begin{lem}
		Suppose that $A := \{ f_\theta > 0 \}$ does not depend on $\theta$ and $\forall x \in A, \theta \mapsto \ln f_\theta(x)$ is continuoulsy differentiable on $\Theta$ with derivative $\theta \mapsto \dot{l}_\theta(x)$.
		Suppose moreover that $\forall \theta \in \Theta$ there exists a neighborhood $V$ of $\theta$ such that $\int \sup_{\theta' \in V} \left( \abs{\dot{l}_\theta(x)}^2 f_\theta(x) \right) \nu(\diff x) < \infty$.
		Then $\mathcal{P}$ is Hellinger differentiable with Hellinger derivative given by $\dot{\xi}_\theta(x) = \frac{1}{2} \dot{l}_\theta(x) \xi_\theta(x) \indic_A(x)$.
	\end{lem}

	\begin{defn}
		Let $\mathcal{P}$ be Hellinger differentiable with Hellinger derivative $\dot{\xi}_\theta$.
		The \textbf{Fisher information matrix} is defined as $\mathcal{I}(\theta) := 4 \int_{\mathsf{X}} \dot{\xi}_\theta(x) \transp{\dot{\xi}_\theta(x)} \nu (\diff x)$.
	\end{defn}

	With the conditions of the previous lemma we have $\mathcal{I}(\theta) = \esp_\theta \left[ \left( \dot{l}_\theta(X) \right)^2 \right]$.
	
	\begin{thm}
		Let $\mathcal{P}$ be Hellinger differentiable with Hellinger derivative $\dot{\xi}_\theta$.
		Let $T = g(X)$ be a scalar statistic such that, for some $\epsilon > 0$, $\sup_{\abs{\theta' - \theta} \leq \epsilon} \esp_{\theta} \left( T^2 \right) < \infty$.
		Define $\psi \colon \theta \to \esp_\theta(T)$.
		Then $\psi$ is differentiable at $\theta$ and, if $\mathcal{I}(\theta)$ is positive definite, we have $\Var_\theta(T) \geq \transp{\dot{\psi}(\theta)} \mathcal{I}(\theta)^{-1} \dot{\psi}(\theta)$.
	\end{thm}
	
	\begin{defn}
		Let $T$ be as in the previous theorem.
		If $\forall \theta \in \Theta, \Var_\theta(T) = \transp{\dot{\psi}(\theta)} \mathcal{I}(\theta)^{-1} \dot{\psi}(\theta)$, we say that $T$ is an efficient estimator of $\psi(\theta)$.
	\end{defn}
